{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "b627a2e6-57ae-4542-b7eb-2a7d96535513"
    }
   },
   "source": [
    "# Shakespeare Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "nbpresent": {
     "id": "837bde82-bf01-48a0-a70c-ae2ff47f6bcf"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.utils.rnn as rnn\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "\n",
    "import shakespeare_data as sh\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "26adeb84-ce2a-438b-a2b1-4df207196897"
    }
   },
   "source": [
    "## Fixed length input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "nbpresent": {
     "id": "07b27339-a14b-45b9-b8c6-2789f04510d8"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1609\n",
      " THE SONNETS\n",
      " by William Shakespeare\n",
      "                      1\n",
      "   From fairest creatures we desire increase,\n",
      "   That thereby beauty's rose might never die,\n",
      "   But as the riper should by time decease,\n",
      "...,\n",
      "   And new pervert a reconciled maid.'\n",
      " THE END\n",
      "\n",
      "Total character count: 5551930\n",
      "Unique character count: 84\n",
      "\n",
      "(5551930,)\n",
      "[12 17 11 20  0  1 45 33 30  1 44 40 39 39 30 45 44]\n",
      "1609\n",
      " THE SONNETS\n"
     ]
    }
   ],
   "source": [
    "# Data - refer to shakespeare_data.py for details\n",
    "corpus = sh.read_corpus()\n",
    "print(\"{}...{}\".format(corpus[:203], corpus[-50:]))\n",
    "print(\"Total character count: {}\".format(len(corpus)))\n",
    "chars, charmap = sh.get_charmap(corpus)\n",
    "charcount = len(chars)\n",
    "print(\"Unique character count: {}\\n\".format(charcount))\n",
    "\n",
    "shakespeare_array = sh.map_corpus(corpus, charmap)\n",
    "print(shakespeare_array.shape)\n",
    "print(shakespeare_array[:17])\n",
    "print(sh.to_text(shakespeare_array[:17],chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "nbpresent": {
     "id": "90146354-7d91-4b07-9bae-e85b345cb068"
    }
   },
   "outputs": [],
   "source": [
    "# Dataset class. Transforme raw text into a set of sequences of fixed length, and extracts inputs and targets\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self,text, seq_len = 200):\n",
    "        n_seq = len(text) // seq_len\n",
    "        text = text[:n_seq * seq_len]\n",
    "        self.data = torch.tensor(text).view(-1,seq_len)\n",
    "    def __getitem__(self,i):\n",
    "        txt = self.data[i]\n",
    "        return txt[:-1],txt[1:]\n",
    "    def __len__(self):\n",
    "        return self.data.size(0)\n",
    "\n",
    "# Collate function. Transform a list of sequences into a batch. Passed as an argument to the DataLoader.\n",
    "# Returns data on the format seq_len x batch_size\n",
    "def collate(seq_list):\n",
    "    '''seq_list is a list; seq_list[0] is a tuple with length 2. First one is inputs, while second one is targets'''\n",
    "#     print(f'len(seq_list): {len(seq_list)}')\n",
    "#     print(f'{seq_list[0][0]}')\n",
    "#     print(f'{seq_list[0][1]}')\n",
    "    inputs = torch.cat([s[0].unsqueeze(1) for s in seq_list],dim=1)\n",
    "    targets = torch.cat([s[1].unsqueeze(1) for s in seq_list],dim=1)\n",
    "#     print(f'inputs: {inputs.shape}')\n",
    "#     print(f'targets: {targets.shape}')\n",
    "    # inputs: torch.Size([199, 64])\n",
    "    # targets: torch.Size([199, 64])\n",
    "    return inputs,targets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "nbpresent": {
     "id": "aee4a856-b92c-4a2c-8977-e344c27b6252"
    }
   },
   "outputs": [],
   "source": [
    "# Model\n",
    "class CharLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self,vocab_size,embed_size,hidden_size, nlayers):\n",
    "        super(CharLanguageModel,self).__init__()\n",
    "        self.vocab_size=vocab_size\n",
    "        self.embed_size = embed_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.nlayers=nlayers\n",
    "        self.embedding = nn.Embedding(vocab_size,embed_size) # Embedding layer\n",
    "        self.rnn = nn.LSTM(input_size = embed_size,hidden_size=hidden_size,num_layers=nlayers) # Recurrent network\n",
    "        self.scoring = nn.Linear(hidden_size,vocab_size) # Projection layer\n",
    "        \n",
    "    def forward(self,seq_batch): #L x N\n",
    "        # returns 3D logits\n",
    "        batch_size = seq_batch.size(1)\n",
    "        embed = self.embedding(seq_batch) #L x N x E\n",
    "        hidden = None\n",
    "        output_lstm,hidden = self.rnn(embed,hidden) #L x N x H\n",
    "        output_lstm_flatten = output_lstm.view(-1,self.hidden_size) #(L*N) x H\n",
    "        output_flatten = self.scoring(output_lstm_flatten) #(L*N) x V\n",
    "        return output_flatten.view(-1,batch_size,self.vocab_size)\n",
    "    \n",
    "    def generate(self,seq, n_words): # L x V\n",
    "        # performs greedy search to extract and return words (one sequence).\n",
    "        generated_words = []\n",
    "        embed = self.embedding(seq).unsqueeze(1) # L x 1 x E\n",
    "        hidden = None\n",
    "        output_lstm, hidden = self.rnn(embed,hidden) # L x 1 x H\n",
    "        output = output_lstm[-1] # 1 x H\n",
    "        scores = self.scoring(output) # 1 x V\n",
    "        _,current_word = torch.max(scores,dim=1) # 1 x 1\n",
    "        generated_words.append(current_word)\n",
    "        if n_words > 1:\n",
    "            for i in range(n_words-1):\n",
    "                embed = self.embedding(current_word).unsqueeze(0) # 1 x 1 x E\n",
    "                output_lstm, hidden = self.rnn(embed,hidden) # 1 x 1 x H\n",
    "                output = output_lstm[0] # 1 x H\n",
    "                scores = self.scoring(output) # V\n",
    "                _,current_word = torch.max(scores,dim=1) # 1\n",
    "                generated_words.append(current_word)\n",
    "        return torch.cat(generated_words,dim=0)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "nbpresent": {
     "id": "63ac3711-8641-4cef-9f95-40b7715d4cfe"
    }
   },
   "outputs": [],
   "source": [
    "def train_epoch(model, optimizer, train_loader, val_loader):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    criterion = criterion.to(DEVICE)\n",
    "    batch_id=0\n",
    "    for inputs,targets in train_loader:\n",
    "        batch_id+=1\n",
    "        inputs = inputs.to(DEVICE)\n",
    "        targets = targets.to(DEVICE)\n",
    "        outputs = model(inputs) # 3D\n",
    "        loss = criterion(outputs.view(-1,outputs.size(2)),targets.view(-1)) # Loss of the flattened outputs\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_id % 100 == 0:\n",
    "            lpw = loss.item()\n",
    "            print(\"At batch\",batch_id)\n",
    "            print(\"Training loss per word:\",lpw)\n",
    "            print(\"Training perplexity :\",np.exp(lpw))\n",
    "    \n",
    "    val_loss = 0\n",
    "    batch_id=0\n",
    "    for inputs,targets in val_loader:\n",
    "        batch_id+=1\n",
    "        inputs = inputs.to(DEVICE)\n",
    "        targets = targets.to(DEVICE)\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs.view(-1,outputs.size(2)),targets.view(-1))\n",
    "        val_loss+=loss.item()\n",
    "    val_lpw = val_loss / batch_id\n",
    "    print(\"\\nValidation loss per word:\",val_lpw)\n",
    "    print(\"Validation perplexity :\",np.exp(val_lpw),\"\\n\")\n",
    "    return val_lpw\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "nbpresent": {
     "id": "e8384b80-5dad-440a-a5dc-40cb032c8ba2"
    }
   },
   "outputs": [],
   "source": [
    "model = CharLanguageModel(charcount,256,256,3)\n",
    "model = model.to(DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=0.001, weight_decay=1e-6)\n",
    "split = 5000000\n",
    "train_dataset = TextDataset(shakespeare_array[:split])\n",
    "val_dataset = TextDataset(shakespeare_array[split:])\n",
    "train_loader = DataLoader(train_dataset, shuffle=True, batch_size=64, collate_fn = collate)\n",
    "val_loader = DataLoader(val_dataset, shuffle=False, batch_size=64, collate_fn = collate, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "nbpresent": {
     "id": "24492dd0-7bdf-4c8f-8987-8299f2fb19fc"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At batch 100\n",
      "Training loss per word: 2.800201177597046\n",
      "Training perplexity : 16.447955398418415\n",
      "At batch 200\n",
      "Training loss per word: 2.1134068965911865\n",
      "Training perplexity : 8.276390112588736\n",
      "At batch 300\n",
      "Training loss per word: 1.8806852102279663\n",
      "Training perplexity : 6.557996929581252\n",
      "\n",
      "Validation loss per word: 1.8039919831031976\n",
      "Validation perplexity : 6.07384582245413 \n",
      "\n",
      "At batch 100\n",
      "Training loss per word: 1.6847354173660278\n",
      "Training perplexity : 5.391024372208103\n",
      "At batch 200\n",
      "Training loss per word: 1.6462814807891846\n",
      "Training perplexity : 5.187653527539739\n",
      "At batch 300\n",
      "Training loss per word: 1.5604981184005737\n",
      "Training perplexity : 4.76119229204712\n",
      "\n",
      "Validation loss per word: 1.5943349461222804\n",
      "Validation perplexity : 4.925052557409207 \n",
      "\n",
      "At batch 100\n",
      "Training loss per word: 1.5046809911727905\n",
      "Training perplexity : 4.502716994683218\n",
      "At batch 200\n",
      "Training loss per word: 1.505882978439331\n",
      "Training perplexity : 4.508132457182136\n",
      "At batch 300\n",
      "Training loss per word: 1.4546234607696533\n",
      "Training perplexity : 4.282870493188299\n",
      "\n",
      "Validation loss per word: 1.5108216418776401\n",
      "Validation perplexity : 4.530451674309748 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    train_epoch(model, optimizer, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "nbpresent": {
     "id": "69b3b526-3400-4c4c-adc4-d9c5aaa2155b"
    }
   },
   "outputs": [],
   "source": [
    "def generate(model, seed,nwords):\n",
    "    seq = sh.map_corpus(seed, charmap)\n",
    "    seq = torch.tensor(seq).to(DEVICE)\n",
    "    out = model.generate(seq,nwords)\n",
    "    return sh.to_text(out.cpu().detach().numpy(),chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "nbpresent": {
     "id": "17bdb4a8-a32b-408a-a374-62ee4436e44c"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uiet\n",
      "   \n"
     ]
    }
   ],
   "source": [
    "print(generate(model, \"To be, or not to be, that is the q\",8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "nbpresent": {
     "id": "cfab54b9-3038-492b-8b0d-16e419359684"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the world the world the words\n",
      "     That the world the streat the world the streans the streans\n",
      "     That the world the streat the world the streans the streans\n",
      "     That the world the streat the world the streans the streans\n",
      "     That the world the streat the world the streans the streans\n",
      "     That the world the streat the world the streans the streans\n",
      "     That the world the streat the world the streans the streans\n",
      "     That the world the streat the world the streans the streans\n",
      "     That the world the streat the world the streans the streans\n",
      "     That the world the streat the world the streans the streans\n",
      "     That the world the streat the world the streans the streans\n",
      "     That the world the streat the world the streans the streans\n",
      "     That the world the streat the world the streans the streans\n",
      "     That the world the streat the world the streans the streans\n",
      "     That the world the streat the world the streans the streans\n",
      "     That the world the streat the world the streans the str\n"
     ]
    }
   ],
   "source": [
    "print(generate(model, \"Richard \", 1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "6a637e4d-cdbb-4a57-972c-94880ed7defa"
    }
   },
   "source": [
    "## Packed sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "nbpresent": {
     "id": "bc2573e1-3e40-4089-b18f-d81a8c13e8f6"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1609\n",
      "\n",
      " THE SONNETS\n",
      "\n",
      " by William Shakespeare\n",
      "\n",
      "                      1\n",
      "\n",
      "   From fairest creatures we desire increase,\n",
      "\n",
      "   That thereby beauty's rose might never die,\n",
      "\n",
      "   But as the riper should by time decease,\n",
      "\n",
      "   His tender heir might bear his memory:\n",
      "\n",
      "   But thou contracted to thine own bright eyes,\n",
      "\n",
      "   Feed'st thy light's flame with self-substantial fuel,\n",
      "\n",
      "114638\n"
     ]
    }
   ],
   "source": [
    "stop_character = charmap['\\n']\n",
    "space_character = charmap[\" \"]\n",
    "lines = np.split(shakespeare_array, np.where(shakespeare_array == stop_character)[0]+1) # split the data in lines\n",
    "shakespeare_lines = []\n",
    "for s in lines:\n",
    "    s_trimmed = np.trim_zeros(s-space_character)+space_character # remove space-only lines\n",
    "    if len(s_trimmed)>1:\n",
    "        shakespeare_lines.append(s)\n",
    "for i in range(10):\n",
    "    print(sh.to_text(shakespeare_lines[i],chars))\n",
    "print(len(shakespeare_lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "nbpresent": {
     "id": "480b92d7-e6d0-4dce-b819-f1afe5336fbe"
    }
   },
   "outputs": [],
   "source": [
    "class LinesDataset(Dataset):\n",
    "    def __init__(self,lines):\n",
    "        self.lines=[torch.tensor(l) for l in lines]\n",
    "    def __getitem__(self,i):\n",
    "        line = self.lines[i]\n",
    "        return line[:-1].to(DEVICE),line[1:].to(DEVICE)\n",
    "    def __len__(self):\n",
    "        return len(self.lines)\n",
    "\n",
    "def collate_lines(seq_list):\n",
    "    inputs,targets = zip(*seq_list)\n",
    "    lens = [len(seq) for seq in inputs]\n",
    "    seq_order = sorted(range(len(lens)), key=lens.__getitem__, reverse=True)\n",
    "    inputs = [inputs[i] for i in seq_order]\n",
    "    targets = [targets[i] for i in seq_order]\n",
    "    return inputs,targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "nbpresent": {
     "id": "6d40b21f-b30d-4194-a363-bdfc7be8866a"
    }
   },
   "outputs": [],
   "source": [
    "# Model that takes packed sequences in training\n",
    "class PackedLanguageModel(nn.Module):\n",
    "    \n",
    "    def __init__(self,vocab_size,embed_size,hidden_size, nlayers, stop):\n",
    "        super(PackedLanguageModel,self).__init__()\n",
    "        self.vocab_size=vocab_size\n",
    "        self.embed_size = embed_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.nlayers=nlayers\n",
    "        self.embedding = nn.Embedding(vocab_size,embed_size)\n",
    "        self.rnn = nn.LSTM(input_size = embed_size,hidden_size=hidden_size,num_layers=nlayers) # 1 layer, batch_size = False\n",
    "        self.scoring = nn.Linear(hidden_size,vocab_size)\n",
    "        self.stop = stop # stop line character (\\n)\n",
    "    \n",
    "    def forward(self,seq_list): # list\n",
    "        batch_size = len(seq_list)\n",
    "        lens = [len(s) for s in seq_list] # lens of all lines (already sorted)\n",
    "#         print(f'lens: {lens}')\n",
    "        bounds = [0]\n",
    "        for l in lens:\n",
    "            bounds.append(bounds[-1]+l) # bounds of all lines in the concatenated sequence\n",
    "#         print(f'seq_list: {len(seq_list)}')\n",
    "#         print(f'seq_list[0]: {seq_list[0].size()}')\n",
    "#         print(f'seq_list[1]: {seq_list[1].size()}')\n",
    "#         print(f'seq_list[2]: {seq_list[2].size()}')\n",
    "        # seq_list[0]: torch.Size([56])\n",
    "        # seq_list[1]: torch.Size([54])\n",
    "        # seq_list[2]: torch.Size([52])\n",
    "        seq_concat = torch.cat(seq_list) # concatenated sequence\n",
    "#         print(f'seq_concat: {seq_concat.size()}')\n",
    "        # seq_concat: torch.Size([2717])\n",
    "        embed_concat = self.embedding(seq_concat) # concatenated embeddings\n",
    "#         print(f'embed_concat: {embed_concat.size()}')\n",
    "        # embed_concat: torch.Size([2717, 256])\n",
    "        embed_list = [embed_concat[bounds[i]:bounds[i+1]] for i in range(batch_size)] # embeddings per line\n",
    "#         print(f'embed_list: {len(embed_list)}')\n",
    "#         print(f'embed_list[0]: {embed_list[0].size()}')\n",
    "        # embed_list[0]: torch.Size([56, 256])\n",
    "#         print(f'embed_list[1]: {embed_list[1].size()}')\n",
    "        \n",
    "        packed_input = rnn.pack_sequence(embed_list) # packed version\n",
    "#         print(f'packed_input: {packed_input}')\n",
    "        hidden = None\n",
    "        output_packed,hidden = self.rnn(packed_input,hidden)\n",
    "        output_padded, _ = rnn.pad_packed_sequence(output_packed) # unpacked output (padded)\n",
    "        output_flatten = torch.cat([output_padded[:lens[i],i] for i in range(batch_size)]) # concatenated output\n",
    "        scores_flatten = self.scoring(output_flatten) # concatenated logits\n",
    "        return scores_flatten # return concatenated logits\n",
    "    \n",
    "    def generate(self,seq, n_words): # L x V\n",
    "        generated_words = []\n",
    "        embed = self.embedding(seq).unsqueeze(1) # L x 1 x E\n",
    "        hidden = None\n",
    "        output_lstm, hidden = self.rnn(embed,hidden) # L x 1 x H\n",
    "        output = output_lstm[-1] # 1 x H\n",
    "        scores = self.scoring(output) # 1 x V\n",
    "        _,current_word = torch.max(scores,dim=1) # 1 x 1\n",
    "        generated_words.append(current_word)\n",
    "        if n_words > 1:\n",
    "            for i in range(n_words-1):\n",
    "                embed = self.embedding(current_word).unsqueeze(0) # 1 x 1 x E\n",
    "                output_lstm, hidden = self.rnn(embed,hidden) # 1 x 1 x H\n",
    "                output = output_lstm[0] # 1 x H\n",
    "                scores = self.scoring(output) # V\n",
    "                _,current_word = torch.max(scores,dim=1) # 1\n",
    "                generated_words.append(current_word)\n",
    "                if current_word[0].item()==self.stop: # If end of line\n",
    "                    break\n",
    "        return torch.cat(generated_words,dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "nbpresent": {
     "id": "e3ec9ba9-4749-472d-83b9-67623f86dc16"
    }
   },
   "outputs": [],
   "source": [
    "def train_epoch_packed(model, optimizer, train_loader, val_loader):\n",
    "    criterion = nn.CrossEntropyLoss(reduction=\"sum\") # sum instead of averaging, to take into account the different lengths\n",
    "    criterion = criterion.to(DEVICE)\n",
    "    batch_id=0\n",
    "    for inputs,targets in train_loader: # lists, presorted, preloaded on GPU\n",
    "        batch_id+=1\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs,torch.cat(targets)) # criterion of the concatenated output\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_id % 100 == 0:\n",
    "            nwords = np.sum(np.array([len(l) for l in inputs]))\n",
    "            lpw = loss.item() / nwords\n",
    "            print(f\"At batch {batch_id}/{len(train_loader)}\")\n",
    "            print(\"Training loss per word:\",lpw)\n",
    "            print(\"Training perplexity :\",np.exp(lpw))\n",
    "    \n",
    "    val_loss = 0\n",
    "    batch_id=0\n",
    "    nwords = 0\n",
    "    for inputs,targets in val_loader:\n",
    "        nwords += np.sum(np.array([len(l) for l in inputs]))\n",
    "        batch_id+=1\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs,torch.cat(targets))\n",
    "        val_loss+=loss.item()\n",
    "    val_lpw = val_loss / nwords\n",
    "    print(\"\\nValidation loss per word:\",val_lpw)\n",
    "    print(\"Validation perplexity :\",np.exp(val_lpw),\"\\n\")\n",
    "    return val_lpw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "nbpresent": {
     "id": "c4921abe-5b73-4758-b6c3-2247b30950b4"
    }
   },
   "outputs": [],
   "source": [
    "model = PackedLanguageModel(charcount,256,256,3, stop=stop_character)\n",
    "model = model.to(DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=0.001, weight_decay=1e-6)\n",
    "split = 100000\n",
    "train_dataset = LinesDataset(shakespeare_lines[:split])\n",
    "val_dataset = LinesDataset(shakespeare_lines[split:])\n",
    "train_loader = DataLoader(train_dataset, shuffle=False, batch_size=64, collate_fn = collate_lines)\n",
    "val_loader = DataLoader(val_dataset, shuffle=False, batch_size=64, collate_fn = collate_lines, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "nbpresent": {
     "id": "003f4d40-8e4b-4eb6-a3a8-34d05a3b4c85"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At batch 100/1563\n",
      "Training loss per word: 1.5194437639977603\n",
      "Training perplexity : 4.5696826660669325\n",
      "At batch 200/1563\n",
      "Training loss per word: 1.6141570480941991\n",
      "Training perplexity : 5.023651441587605\n",
      "At batch 300/1563\n",
      "Training loss per word: 1.471781375264928\n",
      "Training perplexity : 4.356989665773023\n",
      "At batch 400/1563\n",
      "Training loss per word: 1.5731342223163338\n",
      "Training perplexity : 4.821736931187809\n",
      "At batch 500/1563\n",
      "Training loss per word: 1.4621491235149793\n",
      "Training perplexity : 4.315223518385884\n",
      "At batch 600/1563\n",
      "Training loss per word: 1.3245818433872305\n",
      "Training perplexity : 3.7606125011877825\n",
      "At batch 700/1563\n",
      "Training loss per word: 1.387030759833494\n",
      "Training perplexity : 4.0029466796868185\n",
      "At batch 800/1563\n",
      "Training loss per word: 1.4207646704180064\n",
      "Training perplexity : 4.140285183706425\n",
      "At batch 900/1563\n",
      "Training loss per word: 1.5655391565072831\n",
      "Training perplexity : 4.785254241471388\n",
      "At batch 1000/1563\n",
      "Training loss per word: 1.4143404028881554\n",
      "Training perplexity : 4.113772138668992\n",
      "At batch 1100/1563\n",
      "Training loss per word: 1.42881412035295\n",
      "Training perplexity : 4.173746694713618\n",
      "At batch 1200/1563\n",
      "Training loss per word: 1.4184689279329978\n",
      "Training perplexity : 4.130791057314643\n",
      "At batch 1300/1563\n",
      "Training loss per word: 1.4260587658034258\n",
      "Training perplexity : 4.1622623717263\n",
      "At batch 1400/1563\n",
      "Training loss per word: 1.4582112630208333\n",
      "Training perplexity : 4.298264183831532\n",
      "At batch 1500/1563\n",
      "Training loss per word: 1.4372975701800847\n",
      "Training perplexity : 4.209305081024523\n",
      "\n",
      "Validation loss per word: 1.5801289313557672\n",
      "Validation perplexity : 4.8555818076267805 \n",
      "\n",
      "At batch 100/1563\n",
      "Training loss per word: 1.3368079857222845\n",
      "Training perplexity : 3.8068724993166834\n",
      "At batch 200/1563\n",
      "Training loss per word: 1.4698325071654688\n",
      "Training perplexity : 4.34850673634391\n",
      "At batch 300/1563\n",
      "Training loss per word: 1.343702575706091\n",
      "Training perplexity : 3.833210013169496\n",
      "At batch 400/1563\n",
      "Training loss per word: 1.44266282542796\n",
      "Training perplexity : 4.231949770148235\n",
      "At batch 500/1563\n",
      "Training loss per word: 1.309189689483102\n",
      "Training perplexity : 3.7031717770878014\n",
      "At batch 600/1563\n",
      "Training loss per word: 1.221872246838723\n",
      "Training perplexity : 3.393535325422922\n",
      "At batch 700/1563\n",
      "Training loss per word: 1.268338982816522\n",
      "Training perplexity : 3.554942834500109\n",
      "At batch 800/1563\n",
      "Training loss per word: 1.31766981511254\n",
      "Training perplexity : 3.734708668393768\n",
      "At batch 900/1563\n",
      "Training loss per word: 1.483458119755383\n",
      "Training perplexity : 4.408163310189357\n",
      "At batch 1000/1563\n",
      "Training loss per word: 1.3386818019823097\n",
      "Training perplexity : 3.814012566403472\n",
      "At batch 1100/1563\n",
      "Training loss per word: 1.339019904001731\n",
      "Training perplexity : 3.8153023097744043\n",
      "At batch 1200/1563\n",
      "Training loss per word: 1.3592518089854784\n",
      "Training perplexity : 3.8932792952286728\n",
      "At batch 1300/1563\n",
      "Training loss per word: 1.3472813965640293\n",
      "Training perplexity : 3.846952962225594\n",
      "At batch 1400/1563\n",
      "Training loss per word: 1.38527178736844\n",
      "Training perplexity : 3.995911795595693\n",
      "At batch 1500/1563\n",
      "Training loss per word: 1.3513257249735169\n",
      "Training perplexity : 3.862542807357746\n",
      "\n",
      "Validation loss per word: 1.524633180470926\n",
      "Validation perplexity : 4.593458290022142 \n",
      "\n",
      "At batch 100/1563\n",
      "Training loss per word: 1.2715674867021276\n",
      "Training perplexity : 3.566438528215118\n",
      "At batch 200/1563\n",
      "Training loss per word: 1.4096865174217146\n",
      "Training perplexity : 4.094671594647024\n",
      "At batch 300/1563\n",
      "Training loss per word: 1.2735245195298102\n",
      "Training perplexity : 3.5734249996398755\n",
      "At batch 400/1563\n",
      "Training loss per word: 1.380482628750223\n",
      "Training perplexity : 3.976820492311903\n",
      "At batch 500/1563\n",
      "Training loss per word: 1.2272586034349173\n",
      "Training perplexity : 3.4118634333574533\n",
      "At batch 600/1563\n",
      "Training loss per word: 1.172708317138267\n",
      "Training perplexity : 3.230730643714305\n",
      "At batch 700/1563\n",
      "Training loss per word: 1.1980960801761582\n",
      "Training perplexity : 3.3138017000563798\n",
      "At batch 800/1563\n",
      "Training loss per word: 1.25901177213123\n",
      "Training perplexity : 3.5219392884887855\n",
      "At batch 900/1563\n",
      "Training loss per word: 1.4491519553514882\n",
      "Training perplexity : 4.259500736257748\n",
      "At batch 1000/1563\n",
      "Training loss per word: 1.2849710398706897\n",
      "Training perplexity : 3.61456327748177\n",
      "At batch 1100/1563\n",
      "Training loss per word: 1.2883024451102498\n",
      "Training perplexity : 3.626624932498173\n",
      "At batch 1200/1563\n",
      "Training loss per word: 1.3104332530565854\n",
      "Training perplexity : 3.7077797711891085\n",
      "At batch 1300/1563\n",
      "Training loss per word: 1.2980107119188418\n",
      "Training perplexity : 3.6620046348737394\n",
      "At batch 1400/1563\n",
      "Training loss per word: 1.3413598435697809\n",
      "Training perplexity : 3.824240339759917\n",
      "At batch 1500/1563\n",
      "Training loss per word: 1.2935942465572035\n",
      "Training perplexity : 3.6458671797357853\n",
      "\n",
      "Validation loss per word: 1.4877911145548515\n",
      "Validation perplexity : 4.427305300009806 \n",
      "\n",
      "At batch 100/1563\n",
      "Training loss per word: 1.2135083126283128\n",
      "Training perplexity : 3.3652703870364973\n",
      "At batch 200/1563\n",
      "Training loss per word: 1.3763171227968858\n",
      "Training perplexity : 3.960289476800657\n",
      "At batch 300/1563\n",
      "Training loss per word: 1.2363008582807316\n",
      "Training perplexity : 3.4428542744055375\n",
      "At batch 400/1563\n",
      "Training loss per word: 1.3463056403798146\n",
      "Training perplexity : 3.843201104829026\n",
      "At batch 500/1563\n",
      "Training loss per word: 1.1836730884970852\n",
      "Training perplexity : 3.266349787566551\n",
      "At batch 600/1563\n",
      "Training loss per word: 1.140474547833748\n",
      "Training perplexity : 3.1282525184634955\n",
      "At batch 700/1563\n",
      "Training loss per word: 1.160106526378499\n",
      "Training perplexity : 3.1902731062558587\n",
      "At batch 800/1563\n",
      "Training loss per word: 1.2313935798080788\n",
      "Training perplexity : 3.4260006163585275\n",
      "At batch 900/1563\n",
      "Training loss per word: 1.4082553856277706\n",
      "Training perplexity : 4.088815771169206\n",
      "At batch 1000/1563\n",
      "Training loss per word: 1.250293073983028\n",
      "Training perplexity : 3.491366036086059\n",
      "At batch 1100/1563\n",
      "Training loss per word: 1.2526652630663004\n",
      "Training perplexity : 3.4996580477038335\n",
      "At batch 1200/1563\n",
      "Training loss per word: 1.2794825667041843\n",
      "Training perplexity : 3.594779185816338\n",
      "At batch 1300/1563\n",
      "Training loss per word: 1.2690314188672511\n",
      "Training perplexity : 3.557405257513844\n",
      "At batch 1400/1563\n",
      "Training loss per word: 1.3073758627991638\n",
      "Training perplexity : 3.696460953278826\n",
      "At batch 1500/1563\n",
      "Training loss per word: 1.259052734375\n",
      "Training perplexity : 3.5220835579792444\n",
      "\n",
      "Validation loss per word: 1.4603616310085699\n",
      "Validation perplexity : 4.3075169784264515 \n",
      "\n",
      "At batch 100/1563\n",
      "Training loss per word: 1.1771809349045819\n",
      "Training perplexity : 3.245212829488083\n",
      "At batch 200/1563\n",
      "Training loss per word: 1.345306166714793\n",
      "Training perplexity : 3.839361845474278\n",
      "At batch 300/1563\n",
      "Training loss per word: 1.2170834005252489\n",
      "Training perplexity : 3.3773230563057752\n",
      "At batch 400/1563\n",
      "Training loss per word: 1.311052047577122\n",
      "Training perplexity : 3.7100748350081014\n",
      "At batch 500/1563\n",
      "Training loss per word: 1.1560583193440082\n",
      "Training perplexity : 3.177384326052261\n",
      "At batch 600/1563\n",
      "Training loss per word: 1.1095475584317993\n",
      "Training perplexity : 3.0329858350900794\n",
      "At batch 700/1563\n",
      "Training loss per word: 1.1288491424599822\n",
      "Training perplexity : 3.0920958897708686\n",
      "At batch 800/1563\n",
      "Training loss per word: 1.1967566977743167\n",
      "Training perplexity : 3.309366223438878\n",
      "At batch 900/1563\n",
      "Training loss per word: 1.37841394870171\n",
      "Training perplexity : 3.968602226514979\n",
      "At batch 1000/1563\n",
      "Training loss per word: 1.2234406745296784\n",
      "Training perplexity : 3.3988620163703858\n",
      "At batch 1100/1563\n",
      "Training loss per word: 1.2255519841680464\n",
      "Training perplexity : 3.406045647271115\n",
      "At batch 1200/1563\n",
      "Training loss per word: 1.2554579912605932\n",
      "Training perplexity : 3.5094453016128395\n",
      "At batch 1300/1563\n",
      "Training loss per word: 1.243916159002855\n",
      "Training perplexity : 3.469172729332024\n",
      "At batch 1400/1563\n",
      "Training loss per word: 1.283615376297578\n",
      "Training perplexity : 3.60966646567843\n",
      "At batch 1500/1563\n",
      "Training loss per word: 1.230682766154661\n",
      "Training perplexity : 3.4235662336425374\n",
      "\n",
      "Validation loss per word: 1.4364411087701852\n",
      "Validation perplexity : 4.205701517037153 \n",
      "\n",
      "At batch 100/1563\n",
      "Training loss per word: 1.1416507733762598\n",
      "Training perplexity : 3.1319342138067197\n",
      "At batch 200/1563\n",
      "Training loss per word: 1.3214196551751796\n",
      "Training perplexity : 3.748739518862401\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At batch 300/1563\n",
      "Training loss per word: 1.194805051056257\n",
      "Training perplexity : 3.302913808175645\n",
      "At batch 400/1563\n",
      "Training loss per word: 1.2913808781818386\n",
      "Training perplexity : 3.637806456585019\n",
      "At batch 500/1563\n",
      "Training loss per word: 1.132886145725723\n",
      "Training perplexity : 3.104603921470911\n",
      "At batch 600/1563\n",
      "Training loss per word: 1.0910532785292288\n",
      "Training perplexity : 2.977408461986506\n",
      "At batch 700/1563\n",
      "Training loss per word: 1.1083421940355533\n",
      "Training perplexity : 3.0293321843827496\n",
      "At batch 800/1563\n",
      "Training loss per word: 1.182418656425844\n",
      "Training perplexity : 3.262254942526579\n",
      "At batch 900/1563\n",
      "Training loss per word: 1.3628167798052564\n",
      "Training perplexity : 3.907183491610877\n",
      "At batch 1000/1563\n",
      "Training loss per word: 1.205591399094154\n",
      "Training perplexity : 3.338733017999205\n",
      "At batch 1100/1563\n",
      "Training loss per word: 1.206524375258692\n",
      "Training perplexity : 3.3418494298676955\n",
      "At batch 1200/1563\n",
      "Training loss per word: 1.2409034341068592\n",
      "Training perplexity : 3.458736794475722\n",
      "At batch 1300/1563\n",
      "Training loss per word: 1.2232430636980016\n",
      "Training perplexity : 3.398190430779055\n",
      "At batch 1400/1563\n",
      "Training loss per word: 1.2649755070916233\n",
      "Training perplexity : 3.5430059565259726\n",
      "At batch 1500/1563\n",
      "Training loss per word: 1.2054952330508475\n",
      "Training perplexity : 3.33841196069286\n",
      "\n",
      "Validation loss per word: 1.4243409379737724\n",
      "Validation perplexity : 4.15511845935234 \n",
      "\n",
      "At batch 100/1563\n",
      "Training loss per word: 1.115497379794233\n",
      "Training perplexity : 3.051085350049934\n",
      "At batch 200/1563\n",
      "Training loss per word: 1.3019676130165554\n",
      "Training perplexity : 3.676523531007724\n",
      "At batch 300/1563\n",
      "Training loss per word: 1.1807095070321139\n",
      "Training perplexity : 3.2566840235918697\n",
      "At batch 400/1563\n",
      "Training loss per word: 1.27353440745921\n",
      "Training perplexity : 3.5734603335886774\n",
      "At batch 500/1563\n",
      "Training loss per word: 1.1164767683044938\n",
      "Training perplexity : 3.0540750117667916\n",
      "At batch 600/1563\n",
      "Training loss per word: 1.0765566697761193\n",
      "Training perplexity : 2.934557483778846\n",
      "At batch 700/1563\n",
      "Training loss per word: 1.0938634295567888\n",
      "Training perplexity : 2.9857871966765033\n",
      "At batch 800/1563\n",
      "Training loss per word: 1.1634547389971865\n",
      "Training perplexity : 3.200972721224518\n",
      "At batch 900/1563\n",
      "Training loss per word: 1.3477836546073465\n",
      "Training perplexity : 3.848885610596601\n",
      "At batch 1000/1563\n",
      "Training loss per word: 1.1882123399054867\n",
      "Training perplexity : 3.2812102726626065\n",
      "At batch 1100/1563\n",
      "Training loss per word: 1.190004315453793\n",
      "Training perplexity : 3.2870953926607904\n",
      "At batch 1200/1563\n",
      "Training loss per word: 1.2309459104376324\n",
      "Training perplexity : 3.4244672440669657\n",
      "At batch 1300/1563\n",
      "Training loss per word: 1.2059687404414763\n",
      "Training perplexity : 3.3399930977397085\n",
      "At batch 1400/1563\n",
      "Training loss per word: 1.2488446263155997\n",
      "Training perplexity : 3.4863126357714043\n",
      "At batch 1500/1563\n",
      "Training loss per word: 1.1900134898040253\n",
      "Training perplexity : 3.2871255497635055\n",
      "\n",
      "Validation loss per word: 1.4122686132089275\n",
      "Validation perplexity : 4.105258090713156 \n",
      "\n",
      "At batch 100/1563\n",
      "Training loss per word: 1.0971754415126913\n",
      "Training perplexity : 2.995692553845693\n",
      "At batch 200/1563\n",
      "Training loss per word: 1.2905278450654518\n",
      "Training perplexity : 3.634704610383405\n",
      "At batch 300/1563\n",
      "Training loss per word: 1.165091346209224\n",
      "Training perplexity : 3.2062157454809856\n",
      "At batch 400/1563\n",
      "Training loss per word: 1.2587136610088268\n",
      "Training perplexity : 3.5208895156967053\n",
      "At batch 500/1563\n",
      "Training loss per word: 1.1030185523815672\n",
      "Training perplexity : 3.0132479567015586\n",
      "At batch 600/1563\n",
      "Training loss per word: 1.0570062286225124\n",
      "Training perplexity : 2.8777427763082915\n",
      "At batch 700/1563\n",
      "Training loss per word: 1.0809652949545527\n",
      "Training perplexity : 2.9475234077383665\n",
      "At batch 800/1563\n",
      "Training loss per word: 1.14739483897709\n",
      "Training perplexity : 3.149976016430881\n",
      "At batch 900/1563\n",
      "Training loss per word: 1.331097825710497\n",
      "Training perplexity : 3.785196593543181\n",
      "At batch 1000/1563\n",
      "Training loss per word: 1.172632239330774\n",
      "Training perplexity : 3.2304848661595575\n",
      "At batch 1100/1563\n",
      "Training loss per word: 1.178376254656457\n",
      "Training perplexity : 3.249094215768911\n",
      "At batch 1200/1563\n",
      "Training loss per word: 1.2132402840307204\n",
      "Training perplexity : 3.364368519202725\n",
      "At batch 1300/1563\n",
      "Training loss per word: 1.1923451359859298\n",
      "Training perplexity : 3.294798905800308\n",
      "At batch 1400/1563\n",
      "Training loss per word: 1.2344655319618656\n",
      "Training perplexity : 3.436541308292191\n",
      "At batch 1500/1563\n",
      "Training loss per word: 1.1686861096398304\n",
      "Training perplexity : 3.217762073318229\n",
      "\n",
      "Validation loss per word: 1.4091111537707866\n",
      "Validation perplexity : 4.0923163470758706 \n",
      "\n",
      "At batch 100/1563\n",
      "Training loss per word: 1.0869823198079973\n",
      "Training perplexity : 2.9653121934521085\n",
      "At batch 200/1563\n",
      "Training loss per word: 1.2830523126016\n",
      "Training perplexity : 3.607634565635274\n",
      "At batch 300/1563\n",
      "Training loss per word: 1.151154903042066\n",
      "Training perplexity : 3.161842423300141\n",
      "At batch 400/1563\n",
      "Training loss per word: 1.2476200642497326\n",
      "Training perplexity : 3.482046042455213\n",
      "At batch 500/1563\n",
      "Training loss per word: 1.0924962211527818\n",
      "Training perplexity : 2.981707792661812\n",
      "At batch 600/1563\n",
      "Training loss per word: 1.0416330619040215\n",
      "Training perplexity : 2.833841075537333\n",
      "At batch 700/1563\n",
      "Training loss per word: 1.0610026355875966\n",
      "Training perplexity : 2.889266418828605\n",
      "At batch 800/1563\n",
      "Training loss per word: 1.1343216187700964\n",
      "Training perplexity : 3.109063696891527\n",
      "At batch 900/1563\n",
      "Training loss per word: 1.3168081002117638\n",
      "Training perplexity : 3.7314918004947133\n",
      "At batch 1000/1563\n",
      "Training loss per word: 1.1550392940126617\n",
      "Training perplexity : 3.174148140094366\n",
      "At batch 1100/1563\n",
      "Training loss per word: 1.1676438416005042\n",
      "Training perplexity : 3.214410049907935\n",
      "At batch 1200/1563\n",
      "Training loss per word: 1.1965727725271451\n",
      "Training perplexity : 3.308757603410261\n",
      "At batch 1300/1563\n",
      "Training loss per word: 1.1794799207279771\n",
      "Training perplexity : 3.2526821103726045\n",
      "At batch 1400/1563\n",
      "Training loss per word: 1.2253182839082324\n",
      "Training perplexity : 3.4052497465231375\n",
      "At batch 1500/1563\n",
      "Training loss per word: 1.1564213949947033\n",
      "Training perplexity : 3.1785381663869408\n",
      "\n",
      "Validation loss per word: 1.4042775762269804\n",
      "Validation perplexity : 4.072583547106776 \n",
      "\n",
      "At batch 100/1563\n",
      "Training loss per word: 1.0763729105426465\n",
      "Training perplexity : 2.9340182812882807\n",
      "At batch 200/1563\n",
      "Training loss per word: 1.2645129152228782\n",
      "Training perplexity : 3.5413673698072348\n",
      "At batch 300/1563\n",
      "Training loss per word: 1.1448780682708257\n",
      "Training perplexity : 3.1420582168896316\n",
      "At batch 400/1563\n",
      "Training loss per word: 1.2337685665288427\n",
      "Training perplexity : 3.4341469922658314\n",
      "At batch 500/1563\n",
      "Training loss per word: 1.0833735190147211\n",
      "Training perplexity : 2.954630258536627\n",
      "At batch 600/1563\n",
      "Training loss per word: 1.03079605229063\n",
      "Training perplexity : 2.803296517325206\n",
      "At batch 700/1563\n",
      "Training loss per word: 1.051217372506435\n",
      "Training perplexity : 2.8611320623403116\n",
      "At batch 800/1563\n",
      "Training loss per word: 1.1197278970307476\n",
      "Training perplexity : 3.064020360814612\n",
      "At batch 900/1563\n",
      "Training loss per word: 1.3051224383015358\n",
      "Training perplexity : 3.6881406357366977\n",
      "At batch 1000/1563\n",
      "Training loss per word: 1.15002423867412\n",
      "Training perplexity : 3.1582694610262614\n",
      "At batch 1100/1563\n",
      "Training loss per word: 1.1564873793554336\n",
      "Training perplexity : 3.1787479071156346\n",
      "At batch 1200/1563\n",
      "Training loss per word: 1.188478372864804\n",
      "Training perplexity : 3.2820832988633084\n",
      "At batch 1300/1563\n",
      "Training loss per word: 1.1693168204526916\n",
      "Training perplexity : 3.2197921907923086\n",
      "At batch 1400/1563\n",
      "Training loss per word: 1.2188865723219435\n",
      "Training perplexity : 3.38341844385313\n",
      "At batch 1500/1563\n",
      "Training loss per word: 1.1404406117584747\n",
      "Training perplexity : 3.1281463596518724\n",
      "\n",
      "Validation loss per word: 1.396176874079164\n",
      "Validation perplexity : 4.039726024998974 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    train_epoch_packed(model, optimizer, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "nbpresent": {
     "id": "0a4dd227-0a9f-40e8-8efd-1c7aa247d725"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uarrel \n"
     ]
    }
   ],
   "source": [
    "print(generate(model, \"To be, or not to be, that is the q\", 7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "nbpresent": {
     "id": "9ed9f50d-950a-42c5-9752-a59fcde6da0e"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uarrel of TITUS\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(generate(model, \"Richard \", 1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "nbpresent": {
     "id": "70f73fde-91bf-402b-98ac-3745f97ed582"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " company\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(generate(model, \"By new unfolding his imprisoned\", 1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "2ead8ead-da35-4798-a517-1e19ec00d845"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
