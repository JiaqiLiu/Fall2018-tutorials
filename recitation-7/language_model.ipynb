{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shakespeare Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.utils.rnn as rnn\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "\n",
    "import shakespeare_data as sh\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fixed length input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1609\n",
      " THE SONNETS\n",
      " by William Shakespeare\n",
      "                      1\n",
      "   From fairest creatures we desire increase,\n",
      "   That thereby beauty's rose might never die,\n",
      "   But as the riper should by time decease,\n",
      "...,\n",
      "   And new pervert a reconciled maid.'\n",
      " THE END\n",
      "\n",
      "Total character count: 5551930\n",
      "Unique character count: 84\n",
      "\n",
      "(5551930,)\n",
      "[12 17 11 20  0  1 45 33 30  1 44 40 39 39 30 45 44]\n",
      "1609\n",
      " THE SONNETS\n"
     ]
    }
   ],
   "source": [
    "# Data - refer to shakespeare_data.py for details\n",
    "corpus = sh.read_corpus()\n",
    "print(\"{}...{}\".format(corpus[:203], corpus[-50:]))\n",
    "print(\"Total character count: {}\".format(len(corpus)))\n",
    "chars, charmap = sh.get_charmap(corpus)\n",
    "charcount = len(chars)\n",
    "print(\"Unique character count: {}\\n\".format(len(chars)))\n",
    "shakespeare_array = sh.map_corpus(corpus, charmap)\n",
    "print(shakespeare_array.shape)\n",
    "print(shakespeare_array[:17])\n",
    "print(sh.to_text(shakespeare_array[:17],chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset class. Transforme raw text into a set of sequences of fixed length, and extracts inputs and targets\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self,text, seq_len = 200):\n",
    "        n_seq = len(text) // seq_len\n",
    "        text = text[:n_seq * seq_len]\n",
    "        self.data = torch.tensor(text).view(-1,seq_len)\n",
    "    def __getitem__(self,i):\n",
    "        txt = self.data[i]\n",
    "        return txt[:-1],txt[1:]\n",
    "    def __len__(self):\n",
    "        return self.data.size(0)\n",
    "\n",
    "# Collate function. Transform a list of sequences into a batch. Passed as an argument to the DataLoader.\n",
    "# Returns data on the format seq_len x batch_size\n",
    "def collate(seq_list):\n",
    "    print(f'{seq_list[0][0].shape}')\n",
    "    print(f'{seq_list[1][0].unsqueeze(1).shape}')\n",
    "    print(f'{seq_list[0][0].unsqueeze(1)}')\n",
    "    print(f'{seq_list[1][0].unsqueeze(1)}')\n",
    "    my_cat = torch.cat([seq_list[0][0].unsqueeze(1), seq_list[1][0].unsqueeze(1)], dim=1)\n",
    "    print(f'my_cat: {my_cat}')\n",
    "    raise\n",
    "    inputs = torch.cat([s[0].unsqueeze(1) for s in seq_list],dim=1)\n",
    "    targets = torch.cat([s[1].unsqueeze(1) for s in seq_list],dim=1)\n",
    "    return inputs,targets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "class CharLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self,vocab_size,embed_size,hidden_size, nlayers):\n",
    "        super(CharLanguageModel,self).__init__()\n",
    "        self.vocab_size=vocab_size\n",
    "        self.embed_size = embed_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.nlayers=nlayers\n",
    "        self.embedding = nn.Embedding(vocab_size,embed_size) # Embedding layer\n",
    "        self.rnn = nn.LSTM(input_size = embed_size,hidden_size=hidden_size,num_layers=nlayers) # Recurrent network\n",
    "        self.scoring = nn.Linear(hidden_size,vocab_size) # Projection layer\n",
    "        \n",
    "    def forward(self,seq_batch): #L x N\n",
    "        # returns 3D logits\n",
    "        batch_size = seq_batch.size(1)\n",
    "        embed = self.embedding(seq_batch) #L x N x E\n",
    "        hidden = None\n",
    "        output_lstm,hidden = self.rnn(embed,hidden) #L x N x H\n",
    "        output_lstm_flatten = output_lstm.view(-1,self.hidden_size) #(L*N) x H\n",
    "        output_flatten = self.scoring(output_lstm_flatten) #(L*N) x V\n",
    "        return output_flatten.view(-1,batch_size,self.vocab_size)\n",
    "    \n",
    "    def generate(self,seq, n_words): # L x V\n",
    "        # performs greedy search to extract and return words (one sequence).\n",
    "        generated_words = []\n",
    "        embed = self.embedding(seq).unsqueeze(1) # L x 1 x E\n",
    "        hidden = None\n",
    "        output_lstm, hidden = self.rnn(embed,hidden) # L x 1 x H\n",
    "        output = output_lstm[-1] # 1 x H\n",
    "        scores = self.scoring(output) # 1 x V\n",
    "        _,current_word = torch.max(scores,dim=1) # 1 x 1\n",
    "        generated_words.append(current_word)\n",
    "        if n_words > 1:\n",
    "            for i in range(n_words-1):\n",
    "                embed = self.embedding(current_word).unsqueeze(0) # 1 x 1 x E\n",
    "                output_lstm, hidden = self.rnn(embed,hidden) # 1 x 1 x H\n",
    "                output = output_lstm[0] # 1 x H\n",
    "                scores = self.scoring(output) # V\n",
    "                _,current_word = torch.max(scores,dim=1) # 1\n",
    "                generated_words.append(current_word)\n",
    "        return torch.cat(generated_words,dim=0)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, optimizer, train_loader, val_loader):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    criterion = criterion.to(DEVICE)\n",
    "    batch_id=0\n",
    "    for inputs,targets in train_loader:\n",
    "        batch_id+=1\n",
    "        inputs = inputs.to(DEVICE)\n",
    "        targets = targets.to(DEVICE)\n",
    "        outputs = model(inputs) # 3D\n",
    "        loss = criterion(outputs.view(-1,outputs.size(2)),targets.view(-1)) # Loss of the flattened outputs\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_id % 100 == 0:\n",
    "            lpw = loss.item()\n",
    "            print(\"At batch\",batch_id)\n",
    "            print(\"Training loss per word:\",lpw)\n",
    "            print(\"Training perplexity :\",np.exp(lpw))\n",
    "    \n",
    "    val_loss = 0\n",
    "    batch_id=0\n",
    "    for inputs,targets in val_loader:\n",
    "        batch_id+=1\n",
    "        inputs = inputs.to(DEVICE)\n",
    "        targets = targets.to(DEVICE)\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs.view(-1,outputs.size(2)),targets.view(-1))\n",
    "        val_loss+=loss.item()\n",
    "    val_lpw = val_loss / batch_id\n",
    "    print(\"\\nValidation loss per word:\",val_lpw)\n",
    "    print(\"Validation perplexity :\",np.exp(val_lpw),\"\\n\")\n",
    "    return val_lpw\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CharLanguageModel(charcount,256,256,3)\n",
    "model = model.to(DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=0.001, weight_decay=1e-6)\n",
    "split = 5000000\n",
    "train_dataset = TextDataset(shakespeare_array[:split])\n",
    "val_dataset = TextDataset(shakespeare_array[split:])\n",
    "train_loader = DataLoader(train_dataset, shuffle=True, batch_size=64, collate_fn = collate)\n",
    "val_loader = DataLoader(val_dataset, shuffle=False, batch_size=64, collate_fn = collate, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([199])\n",
      "torch.Size([199, 1])\n",
      "tensor([[ 8],\n",
      "        [ 1],\n",
      "        [59],\n",
      "        [70],\n",
      "        [ 1],\n",
      "        [80],\n",
      "        [70],\n",
      "        [76],\n",
      "        [ 1],\n",
      "        [66],\n",
      "        [69],\n",
      "        [70],\n",
      "        [78],\n",
      "        [ 1],\n",
      "        [68],\n",
      "        [80],\n",
      "        [ 1],\n",
      "        [77],\n",
      "        [70],\n",
      "        [64],\n",
      "        [58],\n",
      "        [60],\n",
      "        [25],\n",
      "        [ 0],\n",
      "        [ 1],\n",
      "        [ 1],\n",
      "        [ 1],\n",
      "        [27],\n",
      "        [43],\n",
      "        [26],\n",
      "        [27],\n",
      "        [26],\n",
      "        [39],\n",
      "        [45],\n",
      "        [34],\n",
      "        [40],\n",
      "        [10],\n",
      "        [ 1],\n",
      "        [39],\n",
      "        [70],\n",
      "        [75],\n",
      "        [ 1],\n",
      "        [34],\n",
      "        [10],\n",
      "        [ 1],\n",
      "        [48],\n",
      "        [63],\n",
      "        [56],\n",
      "        [75],\n",
      "        [ 1],\n",
      "        [56],\n",
      "        [73],\n",
      "        [60],\n",
      "        [ 1],\n",
      "        [80],\n",
      "        [70],\n",
      "        [76],\n",
      "        [25],\n",
      "        [ 0],\n",
      "        [ 1],\n",
      "        [ 1],\n",
      "        [ 1],\n",
      "        [43],\n",
      "        [40],\n",
      "        [29],\n",
      "        [30],\n",
      "        [43],\n",
      "        [34],\n",
      "        [32],\n",
      "        [40],\n",
      "        [10],\n",
      "        [ 1],\n",
      "        [38],\n",
      "        [80],\n",
      "        [ 1],\n",
      "        [69],\n",
      "        [56],\n",
      "        [68],\n",
      "        [60],\n",
      "        [ 1],\n",
      "        [64],\n",
      "        [74],\n",
      "        [ 1],\n",
      "        [43],\n",
      "        [70],\n",
      "        [59],\n",
      "        [60],\n",
      "        [73],\n",
      "        [64],\n",
      "        [62],\n",
      "        [70],\n",
      "        [10],\n",
      "        [ 0],\n",
      "        [ 1],\n",
      "        [ 1],\n",
      "        [ 1],\n",
      "        [27],\n",
      "        [43],\n",
      "        [26],\n",
      "        [27],\n",
      "        [26],\n",
      "        [39],\n",
      "        [45],\n",
      "        [34],\n",
      "        [40],\n",
      "        [10],\n",
      "        [ 1],\n",
      "        [ 1],\n",
      "        [ 1],\n",
      "        [ 1],\n",
      "        [ 1],\n",
      "        [ 1],\n",
      "        [ 1],\n",
      "        [ 1],\n",
      "        [ 1],\n",
      "        [ 1],\n",
      "        [ 1],\n",
      "        [ 1],\n",
      "        [ 1],\n",
      "        [ 1],\n",
      "        [ 1],\n",
      "        [ 1],\n",
      "        [ 1],\n",
      "        [ 1],\n",
      "        [ 1],\n",
      "        [ 1],\n",
      "        [ 1],\n",
      "        [45],\n",
      "        [63],\n",
      "        [60],\n",
      "        [ 1],\n",
      "        [78],\n",
      "        [70],\n",
      "        [73],\n",
      "        [74],\n",
      "        [60],\n",
      "        [73],\n",
      "        [ 1],\n",
      "        [78],\n",
      "        [60],\n",
      "        [67],\n",
      "        [58],\n",
      "        [70],\n",
      "        [68],\n",
      "        [60],\n",
      "        [10],\n",
      "        [ 0],\n",
      "        [ 1],\n",
      "        [ 1],\n",
      "        [ 1],\n",
      "        [ 1],\n",
      "        [ 1],\n",
      "        [34],\n",
      "        [ 1],\n",
      "        [63],\n",
      "        [56],\n",
      "        [77],\n",
      "        [60],\n",
      "        [ 1],\n",
      "        [58],\n",
      "        [63],\n",
      "        [56],\n",
      "        [73],\n",
      "        [62],\n",
      "        [60],\n",
      "        [59],\n",
      "        [ 1],\n",
      "        [75],\n",
      "        [63],\n",
      "        [60],\n",
      "        [60],\n",
      "        [ 1],\n",
      "        [69],\n",
      "        [70],\n",
      "        [75],\n",
      "        [ 1],\n",
      "        [75],\n",
      "        [70],\n",
      "        [ 1],\n",
      "        [63],\n",
      "        [56],\n",
      "        [76],\n",
      "        [69],\n",
      "        [75],\n",
      "        [ 1],\n",
      "        [56],\n",
      "        [57],\n",
      "        [70],\n",
      "        [76],\n",
      "        [75],\n",
      "        [ 1],\n",
      "        [68],\n",
      "        [80],\n",
      "        [ 1],\n",
      "        [59],\n",
      "        [70],\n",
      "        [70],\n",
      "        [73],\n",
      "        [74]])\n",
      "tensor([[69],\n",
      "        [ 1],\n",
      "        [75],\n",
      "        [63],\n",
      "        [56],\n",
      "        [75],\n",
      "        [ 1],\n",
      "        [56],\n",
      "        [68],\n",
      "        [64],\n",
      "        [75],\n",
      "        [80],\n",
      "        [ 1],\n",
      "        [78],\n",
      "        [63],\n",
      "        [64],\n",
      "        [58],\n",
      "        [63],\n",
      "        [ 1],\n",
      "        [80],\n",
      "        [70],\n",
      "        [76],\n",
      "        [ 1],\n",
      "        [63],\n",
      "        [56],\n",
      "        [77],\n",
      "        [60],\n",
      "        [ 1],\n",
      "        [68],\n",
      "        [56],\n",
      "        [59],\n",
      "        [60],\n",
      "        [22],\n",
      "        [ 0],\n",
      "        [ 1],\n",
      "        [ 1],\n",
      "        [ 1],\n",
      "        [ 1],\n",
      "        [ 1],\n",
      "        [31],\n",
      "        [70],\n",
      "        [73],\n",
      "        [ 1],\n",
      "        [56],\n",
      "        [75],\n",
      "        [ 1],\n",
      "        [44],\n",
      "        [56],\n",
      "        [64],\n",
      "        [69],\n",
      "        [75],\n",
      "        [ 1],\n",
      "        [38],\n",
      "        [56],\n",
      "        [73],\n",
      "        [80],\n",
      "        [ 5],\n",
      "        [74],\n",
      "        [ 1],\n",
      "        [58],\n",
      "        [63],\n",
      "        [56],\n",
      "        [71],\n",
      "        [60],\n",
      "        [67],\n",
      "        [ 1],\n",
      "        [71],\n",
      "        [73],\n",
      "        [60],\n",
      "        [74],\n",
      "        [60],\n",
      "        [69],\n",
      "        [75],\n",
      "        [67],\n",
      "        [80],\n",
      "        [ 0],\n",
      "        [ 1],\n",
      "        [ 1],\n",
      "        [ 1],\n",
      "        [ 1],\n",
      "        [ 1],\n",
      "        [45],\n",
      "        [63],\n",
      "        [60],\n",
      "        [ 1],\n",
      "        [73],\n",
      "        [64],\n",
      "        [75],\n",
      "        [60],\n",
      "        [74],\n",
      "        [ 1],\n",
      "        [70],\n",
      "        [61],\n",
      "        [ 1],\n",
      "        [68],\n",
      "        [56],\n",
      "        [73],\n",
      "        [73],\n",
      "        [64],\n",
      "        [56],\n",
      "        [62],\n",
      "        [60],\n",
      "        [ 1],\n",
      "        [74],\n",
      "        [63],\n",
      "        [56],\n",
      "        [67],\n",
      "        [67],\n",
      "        [ 1],\n",
      "        [57],\n",
      "        [60],\n",
      "        [ 1],\n",
      "        [74],\n",
      "        [70],\n",
      "        [67],\n",
      "        [60],\n",
      "        [68],\n",
      "        [69],\n",
      "        [64],\n",
      "        [81],\n",
      "        [ 5],\n",
      "        [59],\n",
      "        [10],\n",
      "        [ 0],\n",
      "        [ 1],\n",
      "        [ 1],\n",
      "        [ 1],\n",
      "        [ 1],\n",
      "        [ 1],\n",
      "        [34],\n",
      "        [74],\n",
      "        [ 1],\n",
      "        [69],\n",
      "        [70],\n",
      "        [75],\n",
      "        [ 1],\n",
      "        [75],\n",
      "        [63],\n",
      "        [60],\n",
      "        [ 1],\n",
      "        [37],\n",
      "        [56],\n",
      "        [59],\n",
      "        [80],\n",
      "        [ 1],\n",
      "        [28],\n",
      "        [70],\n",
      "        [69],\n",
      "        [74],\n",
      "        [75],\n",
      "        [56],\n",
      "        [69],\n",
      "        [58],\n",
      "        [60],\n",
      "        [ 1],\n",
      "        [64],\n",
      "        [69],\n",
      "        [ 1],\n",
      "        [75],\n",
      "        [63],\n",
      "        [64],\n",
      "        [74],\n",
      "        [ 1],\n",
      "        [75],\n",
      "        [73],\n",
      "        [70],\n",
      "        [70],\n",
      "        [71],\n",
      "        [25],\n",
      "        [ 0],\n",
      "        [ 1],\n",
      "        [ 1],\n",
      "        [ 1],\n",
      "        [ 1],\n",
      "        [ 1],\n",
      "        [34],\n",
      "        [ 1],\n",
      "        [66],\n",
      "        [69],\n",
      "        [70],\n",
      "        [78],\n",
      "        [ 1],\n",
      "        [74],\n",
      "        [63],\n",
      "        [60],\n",
      "        [ 1],\n",
      "        [64],\n",
      "        [74],\n",
      "        [ 1],\n",
      "        [69],\n",
      "        [70],\n",
      "        [75],\n",
      "        [22],\n",
      "        [ 1],\n",
      "        [61],\n",
      "        [70],\n",
      "        [73],\n",
      "        [ 1],\n",
      "        [75]])\n",
      "my_cat: tensor([[ 8, 69],\n",
      "        [ 1,  1],\n",
      "        [59, 75],\n",
      "        [70, 63],\n",
      "        [ 1, 56],\n",
      "        [80, 75],\n",
      "        [70,  1],\n",
      "        [76, 56],\n",
      "        [ 1, 68],\n",
      "        [66, 64],\n",
      "        [69, 75],\n",
      "        [70, 80],\n",
      "        [78,  1],\n",
      "        [ 1, 78],\n",
      "        [68, 63],\n",
      "        [80, 64],\n",
      "        [ 1, 58],\n",
      "        [77, 63],\n",
      "        [70,  1],\n",
      "        [64, 80],\n",
      "        [58, 70],\n",
      "        [60, 76],\n",
      "        [25,  1],\n",
      "        [ 0, 63],\n",
      "        [ 1, 56],\n",
      "        [ 1, 77],\n",
      "        [ 1, 60],\n",
      "        [27,  1],\n",
      "        [43, 68],\n",
      "        [26, 56],\n",
      "        [27, 59],\n",
      "        [26, 60],\n",
      "        [39, 22],\n",
      "        [45,  0],\n",
      "        [34,  1],\n",
      "        [40,  1],\n",
      "        [10,  1],\n",
      "        [ 1,  1],\n",
      "        [39,  1],\n",
      "        [70, 31],\n",
      "        [75, 70],\n",
      "        [ 1, 73],\n",
      "        [34,  1],\n",
      "        [10, 56],\n",
      "        [ 1, 75],\n",
      "        [48,  1],\n",
      "        [63, 44],\n",
      "        [56, 56],\n",
      "        [75, 64],\n",
      "        [ 1, 69],\n",
      "        [56, 75],\n",
      "        [73,  1],\n",
      "        [60, 38],\n",
      "        [ 1, 56],\n",
      "        [80, 73],\n",
      "        [70, 80],\n",
      "        [76,  5],\n",
      "        [25, 74],\n",
      "        [ 0,  1],\n",
      "        [ 1, 58],\n",
      "        [ 1, 63],\n",
      "        [ 1, 56],\n",
      "        [43, 71],\n",
      "        [40, 60],\n",
      "        [29, 67],\n",
      "        [30,  1],\n",
      "        [43, 71],\n",
      "        [34, 73],\n",
      "        [32, 60],\n",
      "        [40, 74],\n",
      "        [10, 60],\n",
      "        [ 1, 69],\n",
      "        [38, 75],\n",
      "        [80, 67],\n",
      "        [ 1, 80],\n",
      "        [69,  0],\n",
      "        [56,  1],\n",
      "        [68,  1],\n",
      "        [60,  1],\n",
      "        [ 1,  1],\n",
      "        [64,  1],\n",
      "        [74, 45],\n",
      "        [ 1, 63],\n",
      "        [43, 60],\n",
      "        [70,  1],\n",
      "        [59, 73],\n",
      "        [60, 64],\n",
      "        [73, 75],\n",
      "        [64, 60],\n",
      "        [62, 74],\n",
      "        [70,  1],\n",
      "        [10, 70],\n",
      "        [ 0, 61],\n",
      "        [ 1,  1],\n",
      "        [ 1, 68],\n",
      "        [ 1, 56],\n",
      "        [27, 73],\n",
      "        [43, 73],\n",
      "        [26, 64],\n",
      "        [27, 56],\n",
      "        [26, 62],\n",
      "        [39, 60],\n",
      "        [45,  1],\n",
      "        [34, 74],\n",
      "        [40, 63],\n",
      "        [10, 56],\n",
      "        [ 1, 67],\n",
      "        [ 1, 67],\n",
      "        [ 1,  1],\n",
      "        [ 1, 57],\n",
      "        [ 1, 60],\n",
      "        [ 1,  1],\n",
      "        [ 1, 74],\n",
      "        [ 1, 70],\n",
      "        [ 1, 67],\n",
      "        [ 1, 60],\n",
      "        [ 1, 68],\n",
      "        [ 1, 69],\n",
      "        [ 1, 64],\n",
      "        [ 1, 81],\n",
      "        [ 1,  5],\n",
      "        [ 1, 59],\n",
      "        [ 1, 10],\n",
      "        [ 1,  0],\n",
      "        [ 1,  1],\n",
      "        [ 1,  1],\n",
      "        [ 1,  1],\n",
      "        [45,  1],\n",
      "        [63,  1],\n",
      "        [60, 34],\n",
      "        [ 1, 74],\n",
      "        [78,  1],\n",
      "        [70, 69],\n",
      "        [73, 70],\n",
      "        [74, 75],\n",
      "        [60,  1],\n",
      "        [73, 75],\n",
      "        [ 1, 63],\n",
      "        [78, 60],\n",
      "        [60,  1],\n",
      "        [67, 37],\n",
      "        [58, 56],\n",
      "        [70, 59],\n",
      "        [68, 80],\n",
      "        [60,  1],\n",
      "        [10, 28],\n",
      "        [ 0, 70],\n",
      "        [ 1, 69],\n",
      "        [ 1, 74],\n",
      "        [ 1, 75],\n",
      "        [ 1, 56],\n",
      "        [ 1, 69],\n",
      "        [34, 58],\n",
      "        [ 1, 60],\n",
      "        [63,  1],\n",
      "        [56, 64],\n",
      "        [77, 69],\n",
      "        [60,  1],\n",
      "        [ 1, 75],\n",
      "        [58, 63],\n",
      "        [63, 64],\n",
      "        [56, 74],\n",
      "        [73,  1],\n",
      "        [62, 75],\n",
      "        [60, 73],\n",
      "        [59, 70],\n",
      "        [ 1, 70],\n",
      "        [75, 71],\n",
      "        [63, 25],\n",
      "        [60,  0],\n",
      "        [60,  1],\n",
      "        [ 1,  1],\n",
      "        [69,  1],\n",
      "        [70,  1],\n",
      "        [75,  1],\n",
      "        [ 1, 34],\n",
      "        [75,  1],\n",
      "        [70, 66],\n",
      "        [ 1, 69],\n",
      "        [63, 70],\n",
      "        [56, 78],\n",
      "        [76,  1],\n",
      "        [69, 74],\n",
      "        [75, 63],\n",
      "        [ 1, 60],\n",
      "        [56,  1],\n",
      "        [57, 64],\n",
      "        [70, 74],\n",
      "        [76,  1],\n",
      "        [75, 69],\n",
      "        [ 1, 70],\n",
      "        [68, 75],\n",
      "        [80, 22],\n",
      "        [ 1,  1],\n",
      "        [59, 61],\n",
      "        [70, 70],\n",
      "        [70, 73],\n",
      "        [73,  1],\n",
      "        [74, 75]])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "No active exception to reraise",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-326-6a1f404d4be6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-324-528c47b36f6a>\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(model, optimizer, train_loader, val_loader)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mcriterion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mbatch_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtargets\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0mbatch_id\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workspace/my_venv/venv_3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    312\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# same-process loading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 314\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    315\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpin_memory_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-322-e2e785463a0a>\u001b[0m in \u001b[0;36mcollate\u001b[0;34m(seq_list)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mmy_cat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mseq_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'my_cat: {my_cat}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mseq_list\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mseq_list\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: No active exception to reraise"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    train_epoch(model, optimizer, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, seed,nwords):\n",
    "    seq = sh.map_corpus(seed, charmap)\n",
    "    seq = torch.tensor(seq).to(DEVICE)\n",
    "    out = model.generate(seq,nwords)\n",
    "    return sh.to_text(out.cpu().detach().numpy(),chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hhhhhhhh\n"
     ]
    }
   ],
   "source": [
    "print(generate(model, \"To be, or not to be, that is the q\",8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n"
     ]
    }
   ],
   "source": [
    "print(generate(model, \"Richard \", 1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packed sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1609\n",
      "\n",
      " THE SONNETS\n",
      "\n",
      " by William Shakespeare\n",
      "\n",
      "                      1\n",
      "\n",
      "   From fairest creatures we desire increase,\n",
      "\n",
      "   That thereby beauty's rose might never die,\n",
      "\n",
      "   But as the riper should by time decease,\n",
      "\n",
      "   His tender heir might bear his memory:\n",
      "\n",
      "   But thou contracted to thine own bright eyes,\n",
      "\n",
      "   Feed'st thy light's flame with self-substantial fuel,\n",
      "\n",
      "114638\n"
     ]
    }
   ],
   "source": [
    "stop_character = charmap['\\n']\n",
    "space_character = charmap[\" \"]\n",
    "lines = np.split(shakespeare_array, np.where(shakespeare_array == stop_character)[0]+1) # split the data in lines\n",
    "shakespeare_lines = []\n",
    "for s in lines:\n",
    "    s_trimmed = np.trim_zeros(s-space_character)+space_character # remove space-only lines\n",
    "    if len(s_trimmed)>1:\n",
    "        shakespeare_lines.append(s)\n",
    "for i in range(10):\n",
    "    print(sh.to_text(shakespeare_lines[i],chars))\n",
    "print(len(shakespeare_lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinesDataset(Dataset):\n",
    "    def __init__(self,lines):\n",
    "        self.lines=[torch.tensor(l) for l in lines]\n",
    "    def __getitem__(self,i):\n",
    "        line = self.lines[i]\n",
    "        return line[:-1].to(DEVICE),line[1:].to(DEVICE)\n",
    "    def __len__(self):\n",
    "        return len(self.lines)\n",
    "\n",
    "def collate_lines(seq_list):\n",
    "    inputs,targets = zip(*seq_list)\n",
    "    lens = [len(seq) for seq in inputs]\n",
    "    seq_order = sorted(range(len(lens)), key=lens.__getitem__, reverse=True)\n",
    "    inputs = [inputs[i] for i in seq_order]\n",
    "    targets = [targets[i] for i in seq_order]\n",
    "    return inputs,targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model that takes packed sequences in training\n",
    "class PackedLanguageModel(nn.Module):\n",
    "    \n",
    "    def __init__(self,vocab_size,embed_size,hidden_size, nlayers, stop):\n",
    "        super(PackedLanguageModel,self).__init__()\n",
    "        self.vocab_size=vocab_size\n",
    "        self.embed_size = embed_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.nlayers=nlayers\n",
    "        self.embedding = nn.Embedding(vocab_size,embed_size)\n",
    "        self.rnn = nn.LSTM(input_size = embed_size,hidden_size=hidden_size,num_layers=nlayers) # 1 layer, batch_size = False\n",
    "        self.scoring = nn.Linear(hidden_size,vocab_size)\n",
    "        self.stop = stop # stop line character (\\n)\n",
    "    \n",
    "    def forward(self,seq_list): # list\n",
    "        batch_size = len(seq_list)\n",
    "        lens = [len(s) for s in seq_list] # lens of all lines (already sorted)\n",
    "        print(f'lens: {lens}')\n",
    "        bounds = [0]\n",
    "        for l in lens:\n",
    "            bounds.append(bounds[-1]+l) # bounds of all lines in the concatenated sequence\n",
    "        print(f'seq_list: {len(seq_list)}')\n",
    "        print(f'seq_list[0]: {seq_list[0].size()}')\n",
    "        print(f'seq_list[1]: {seq_list[1].size()}')\n",
    "        print(f'seq_list[2]: {seq_list[2].size()}')\n",
    "        # seq_list[0]: torch.Size([56])\n",
    "        # seq_list[1]: torch.Size([54])\n",
    "        # seq_list[2]: torch.Size([52])\n",
    "        seq_concat = torch.cat(seq_list) # concatenated sequence\n",
    "        print(f'seq_concat: {seq_concat.size()}')\n",
    "        # seq_concat: torch.Size([2717])\n",
    "        embed_concat = self.embedding(seq_concat) # concatenated embeddings\n",
    "        print(f'embed_concat: {embed_concat.size()}')\n",
    "        # embed_concat: torch.Size([2717, 256])\n",
    "        embed_list = [embed_concat[bounds[i]:bounds[i+1]] for i in range(batch_size)] # embeddings per line\n",
    "        print(f'embed_list: {len(embed_list)}')\n",
    "        print(f'embed_list[0]: {embed_list[0].size()}')\n",
    "        # embed_list[0]: torch.Size([56, 256])\n",
    "        print(f'embed_list[1]: {embed_list[1].size()}')\n",
    "        \n",
    "        packed_input = rnn.pack_sequence(embed_list) # packed version\n",
    "        print(f'packed_input: {packed_input}')\n",
    "        hidden = None\n",
    "        output_packed,hidden = self.rnn(packed_input,hidden)\n",
    "        output_padded, _ = rnn.pad_packed_sequence(output_packed) # unpacked output (padded)\n",
    "        output_flatten = torch.cat([output_padded[:lens[i],i] for i in range(batch_size)]) # concatenated output\n",
    "        scores_flatten = self.scoring(output_flatten) # concatenated logits\n",
    "        return scores_flatten # return concatenated logits\n",
    "    \n",
    "    def generate(self,seq, n_words): # L x V\n",
    "        generated_words = []\n",
    "        embed = self.embedding(seq).unsqueeze(1) # L x 1 x E\n",
    "        hidden = None\n",
    "        output_lstm, hidden = self.rnn(embed,hidden) # L x 1 x H\n",
    "        output = output_lstm[-1] # 1 x H\n",
    "        scores = self.scoring(output) # 1 x V\n",
    "        _,current_word = torch.max(scores,dim=1) # 1 x 1\n",
    "        generated_words.append(current_word)\n",
    "        if n_words > 1:\n",
    "            for i in range(n_words-1):\n",
    "                embed = self.embedding(current_word).unsqueeze(0) # 1 x 1 x E\n",
    "                output_lstm, hidden = self.rnn(embed,hidden) # 1 x 1 x H\n",
    "                output = output_lstm[0] # 1 x H\n",
    "                scores = self.scoring(output) # V\n",
    "                _,current_word = torch.max(scores,dim=1) # 1\n",
    "                generated_words.append(current_word)\n",
    "                if current_word[0].item()==self.stop: # If end of line\n",
    "                    break\n",
    "        return torch.cat(generated_words,dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch_packed(model, optimizer, train_loader, val_loader):\n",
    "    criterion = nn.CrossEntropyLoss(reduction=\"sum\") # sum instead of averaging, to take into account the different lengths\n",
    "    criterion = criterion.to(DEVICE)\n",
    "    batch_id=0\n",
    "    for inputs,targets in train_loader: # lists, presorted, preloaded on GPU\n",
    "        batch_id+=1\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs,torch.cat(targets)) # criterion of the concatenated output\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_id % 100 == 0:\n",
    "            nwords = np.sum(np.array([len(l) for l in inputs]))\n",
    "            lpw = loss.item() / nwords\n",
    "            print(\"At batch\",batch_id)\n",
    "            print(\"Training loss per word:\",lpw)\n",
    "            print(\"Training perplexity :\",np.exp(lpw))\n",
    "    \n",
    "    val_loss = 0\n",
    "    batch_id=0\n",
    "    nwords = 0\n",
    "    for inputs,targets in val_loader:\n",
    "        nwords += np.sum(np.array([len(l) for l in inputs]))\n",
    "        batch_id+=1\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs,torch.cat(targets))\n",
    "        val_loss+=loss.item()\n",
    "    val_lpw = val_loss / nwords\n",
    "    print(\"\\nValidation loss per word:\",val_lpw)\n",
    "    print(\"Validation perplexity :\",np.exp(val_lpw),\"\\n\")\n",
    "    return val_lpw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PackedLanguageModel(charcount,256,256,3, stop=stop_character)\n",
    "model = model.to(DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=0.001, weight_decay=1e-6)\n",
    "split = 100000\n",
    "train_dataset = LinesDataset(shakespeare_lines[:split])\n",
    "val_dataset = LinesDataset(shakespeare_lines[split:])\n",
    "train_loader = DataLoader(train_dataset, shuffle=False, batch_size=64, collate_fn = collate_lines)\n",
    "val_loader = DataLoader(val_dataset, shuffle=False, batch_size=64, collate_fn = collate_lines, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lens: [56, 54, 52, 52, 51, 51, 50, 50, 50, 49, 49, 49, 48, 48, 48, 48, 48, 48, 47, 47, 47, 47, 47, 47, 46, 46, 46, 46, 46, 46, 45, 45, 45, 45, 44, 44, 44, 44, 44, 44, 44, 44, 44, 43, 43, 42, 42, 41, 41, 41, 41, 40, 40, 39, 38, 37, 23, 23, 23, 23, 23, 23, 12, 4]\n",
      "seq_list: 64\n",
      "seq_list[0]: torch.Size([56])\n",
      "seq_list[1]: torch.Size([54])\n",
      "seq_list[2]: torch.Size([52])\n",
      "seq_concat: torch.Size([2717])\n",
      "embed_concat: torch.Size([2717, 256])\n",
      "embed_list: 64\n",
      "embed_list[0]: torch.Size([56, 256])\n",
      "embed_list[1]: torch.Size([54, 256])\n",
      "packed_input: PackedSequence(data=tensor([[ 0.7654, -0.1832, -1.5812,  ...,  0.1848,  0.4273, -0.7515],\n",
      "        [ 0.7654, -0.1832, -1.5812,  ...,  0.1848,  0.4273, -0.7515],\n",
      "        [ 0.7654, -0.1832, -1.5812,  ...,  0.1848,  0.4273, -0.7515],\n",
      "        ...,\n",
      "        [ 0.1681,  0.0924,  0.0588,  ..., -0.1643,  0.2586,  0.3805],\n",
      "        [-0.1080, -0.3240,  1.0240,  ...,  2.1452,  0.0503, -0.2296],\n",
      "        [ 0.5822, -1.8107,  0.7466,  ...,  0.3248, -0.2208,  0.8685]],\n",
      "       grad_fn=<PackPaddedBackward>), batch_sizes=tensor([64, 64, 64, 64, 63, 63, 63, 63, 63, 63, 63, 63, 62, 62, 62, 62, 62, 62,\n",
      "        62, 62, 62, 62, 62, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56,\n",
      "        56, 55, 54, 53, 51, 47, 45, 43, 34, 30, 24, 18, 12,  9,  6,  4,  2,  2,\n",
      "         1,  1], grad_fn=<PackPaddedBackward>))\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "No active exception to reraise",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-294-4fcd402c5ace>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mtrain_epoch_packed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-292-f0adc0376685>\u001b[0m in \u001b[0;36mtrain_epoch_packed\u001b[0;34m(model, optimizer, train_loader, val_loader)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtargets\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# lists, presorted, preloaded on GPU\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mbatch_id\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# criterion of the concatenated output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workspace/my_venv/venv_3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-291-82ddb9185efc>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, seq_list)\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mpacked_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membed_list\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# packed version\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'packed_input: {packed_input}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m         \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0moutput_packed\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpacked_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: No active exception to reraise"
     ]
    }
   ],
   "source": [
    "for i in range(1):\n",
    "    train_epoch_packed(model, optimizer, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(generate(model, \"To be, or not to be, that is the q\",20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(generate(model, \"Richard \", 1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
